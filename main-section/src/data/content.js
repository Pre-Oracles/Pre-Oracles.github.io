export const sampleData = {
  resume: {
    title: "Resume",
    content: {
      experience: [
        { 
          role: "Full-Stack Developer", 
          company: "self-employed", 
          period: "2020-Now", 
          description: "Learn and utilize new and interesting skills out there, combining them into projects that can been seen in the \"things i've made section\"" 
        },
        { 
          role: "Intern", 
          company: "Climind", 
          period: "2023-2024", 
          description: "Developed scripts to obtain open & high value data for the Climind AI." 
        }
      ],
      skills: ["JavaScript", "React", "Node.js", "Python", "CSS", "HTML", "(Native) Chinese", "(Work Professiency) Tamil"],
      education: [
        { 
          degree: "Math-CS undergraduate degree", 
          institution: "UCSD", 
          year: "2025-2029" 
        },
        { 
          degree: "A-Levels", 
          institution: "St Pauls School", 
          year: "2023-2025" 
        },
        {
          degree: "GCSEs",
          institution: "Caterham School",
          year: "2020-2023"
        }
      ]
      
    },
    previewImage: "/resources/resume.jpg"
  },
  projects: {
    title: "Things That I've Made",
    content: [
      { 
        name: "3D Fractal Simulator", 
        tech: "Unity, C#, *Math", 
        description: "Full 3 Dimensional Julia Set Generator and Simulator",
        image:"/resources/julia_set.png",
        images: [
          "/resources/julia_set.png",
          "/resources/julia_set1.png",
          "/resources/julia_set2.png"
        ]
      },
      { 
        name: "Political Speech Classifier", 
        tech: "Python, Matrices", 
        description: "Encoder-Only model which classify the enter political speech into the political party in the US.",
        image:"/resources/political_speech.png",
        images: [
          "/resources/political_speech.png",
          "/resources/political_speech1.png",
          "/resources/political_speech2.png",
        ]
      },
      { 
        name: "2D Pixel Plateformer", 
        tech: "Godot, GDscript", 
        description: "A self drawn, self coded 2D platformer",
        image:"/resources/godot_game.png",
      },
      { 
        name: "This very website", 
        tech: "html, css, js, React", 
        description: "written from the very scratch, where people can learn more about me",
        image:"/resources/this_website.png",
        images: [
          "/resources/this_website.png",
          "/resources/journey2.png"
        ]
      }
    ],
    previewImage: "/resources/julia_set.png"
  },
  thoughts: {
    title: "Thoughts",
    content: [
      { 
        title: "Chinese nationalization modernization is actually quite intersting", 
        date: "2024-09-03", 
        excerpt: "Chinese intellectuals simultaneously had to invent the concept of China as a modern nation-state while defending it against foreign encroachment, essentially creating and protecting a national identity in the same historical moment. This type of attempts to speedrun modernization is so chaotic, and thus interesting. Similiar situation can be seen with the Ottoman Empire before WW1" ,
        image: "/resources/yatsen.jpg",
        readTime: "10 min read",
        tags: ["chinese","nationalism", "colonialism"],
        content: `
# Chinese Nationalization Modernization is Actually Quite Interesting

The story of Chinese modernization presents one of history's most fascinating paradoxes: how do you simultaneously invent and defend a national identity? Chinese intellectuals of the late 19th and early 20th centuries found themselves in this exact predicament, forced to conceptualize China as a modern nation-state while desperately protecting it from foreign encroachment.

## The Double Bind of Modernization

Unlike Western nations that had centuries to develop their national identities organically, China faced what we might call a "modernization speedrun" - an accelerated process born out of necessity rather than natural evolution. The Opium Wars (1839-1842, 1856-1860) had shattered China's traditional worldview and forced a reckoning with Western power.

Chinese intellectuals like Liang Qichao, Yan Fu, and later figures like Chen Duxiu found themselves caught between two impossible demands:

### Creating National Identity
- **Defining "China"**: What exactly was China? An empire? A civilization? A race? The traditional concept of "All under Heaven" (tianxia) didn't translate to modern geopolitics
- **Imagining the Nation**: Benedict Anderson's concept of "imagined communities" becomes particularly relevant here - China had to be imagined as a nation before it could be defended as one
- **Cultural Renaissance**: The New Culture Movement attempted to define Chinese modernity while preserving cultural essence

### Defending Against Encroachment
- **Territorial Integrity**: Foreign concessions, spheres of influence, and extraterritorial rights were carving up Chinese sovereignty
- **Economic Independence**: The unequal treaties and foreign control of customs created economic dependency
- **Cultural Resistance**: Missionaries and Western education challenged traditional Confucian values

## The Ottoman Parallel

The Ottoman Empire faced remarkably similar challenges during the Tanzimat period (1839-1876) and beyond. Like China, the Ottomans attempted to modernize their institutions while maintaining their territorial integrity and cultural identity.

### Shared Challenges:
- **Administrative Reform**: Both empires needed to create modern bureaucracies while maintaining legitimacy
- **Military Modernization**: Adopting Western military technology and organization while preserving sovereignty
- **Constitutional Monarchy**: Attempts to balance traditional authority with modern governance
- **Educational Reform**: Creating modern educational systems while preserving cultural values

### Key Differences:
The Ottoman Empire's location made it more directly involved in European balance of power politics, while China's geographic isolation provided both protection and disadvantage. The Ottoman's Islamic identity also provided a different foundation for modernization compared to China's Confucian heritage.

## The Chaos of Speedrun Modernization

What makes these historical moments so fascinating is their inherent chaos. Normal historical development allows societies to gradually adapt institutions, values, and identities. But when forced to modernize rapidly:

### Contradictory Impulses Emerge:
- **Selective Adoption**: Wanting Western technology but rejecting Western values
- **Cultural Schizophrenia**: Simultaneous embrace and rejection of tradition
- **Political Experimentation**: Rapid cycling through different governmental forms

### Unintended Consequences:
- **Identity Crisis**: Multiple competing visions of national identity
- **Social Fragmentation**: Different classes and regions modernizing at different rates
- **Revolutionary Potential**: Instability creating conditions for radical change

## The Intellectual Battlefield

Chinese intellectuals became warriors on an intellectual battlefield, fighting multiple fronts simultaneously:

### Against Foreign Powers:
- Developing theories of national sovereignty and self-determination
- Creating narratives of Chinese exceptionalism and cultural superiority
- Mobilizing public opinion against foreign encroachment

### Against Tradition:
- Critiquing Confucian hierarchy and gender relations
- Promoting scientific rationalism over traditional cosmology
- Advocating for vernacular literature over classical Chinese

### Against Each Other:
- Liberals vs. Revolutionaries on the pace of change
- Westernizers vs. Traditionalists on cultural preservation
- Federalists vs. Centralists on political organization

## Modern Resonances

This historical pattern continues to echo in contemporary China's development:

### Contemporary Parallels:
- **Economic Modernization**: Rapid economic growth while maintaining political control
- **Cultural Assertion**: Promoting "Chinese characteristics" in socialism
- **Technological Sovereignty**: Developing independent tech capabilities while engaging globally

### Lessons for Other Nations:
The Chinese and Ottoman experiences offer insights for any society undergoing rapid modernization:
- **Identity Flexibility**: National identity can be more malleable than assumed
- **Selective Modernization**: It's possible to adopt some modern elements while rejecting others
- **Crisis as Catalyst**: External pressure can accelerate internal transformation

## Conclusion

The chaos of speedrun modernization creates fascinating historical moments precisely because it compresses centuries of normal development into decades. The Chinese case is particularly interesting because it shows how intellectual elites can simultaneously create and defend national concepts that didn't previously exist.

This process wasn't unique to China - the Ottoman Empire, Meiji Japan, and even contemporary developing nations face similar challenges. But the Chinese case remains particularly compelling because of its scale, its success in maintaining territorial integrity, and its ongoing relevance to understanding modern China's approach to development and sovereignty.

The next time someone asks why Chinese modernization seems contradictory or chaotic, remember: they were essentially trying to build and defend a house at the same time, while the storm was already raging.
  `
      },
      { 
        title: "The Future of AI Agents", 
        date: "2024-08-01", 
        excerpt: "It is not just going to be transformers, because whenever a technology is only being slightly refined under heavy competition, it suggest the end is near for it (even if it takes 50 years), new paradigms, new technology will emerge eventually and overtake transformers like how they overtook the CNNs and their predecessors. " ,
        readTime: "12 min read",
        tags: ["tech", "ML","DL","Transformers"],
        content: `
# The Future of AI Agents

The current AI landscape is dominated by transformers, and everyone seems convinced this architecture will define artificial intelligence for the foreseeable future. But history suggests otherwise. When a technology becomes subject to incremental refinements under intense competition, it often signals that a paradigm shift is approaching—even if that shift takes decades to fully materialize.

## The Pattern of Technological Succession

### The CNN Era and Its Decline
Convolutional Neural Networks (CNNs) once seemed unshakeable. From AlexNet's breakthrough in 2012 to ResNet's innovations in 2015, CNNs dominated computer vision. The incremental improvements—batch normalization, skip connections, attention mechanisms—seemed to promise endless progress.

But by 2017, the Vision Transformer (ViT) papers began showing that transformers could match or exceed CNN performance on image tasks. The writing was on the wall, even though many refused to read it.

### Pre-Transformer Architectures
Before transformers, we had:
- **RNNs and LSTMs**: Dominated sequence modeling until attention mechanisms made them obsolete
- **Encoder-Decoder Architectures**: Powerful but limited by their sequential nature
- **Convolutional Sequence Models**: Attempts to parallelize sequence processing

Each seemed revolutionary until the next paradigm emerged.

## Signs That Transformer Dominance May Be Waning

### Diminishing Returns on Scale
The returns on simply making transformers larger are showing clear signs of diminishing marginal utility:

- **Parameter Scaling**: GPT-3 to GPT-4 required massive increases in parameters for modest performance gains
- **Compute Costs**: Training costs are growing exponentially while improvements are becoming linear
- **Data Limitations**: High-quality training data is becoming scarce, forcing models to train on synthetic or lower-quality data

### Architectural Stagnation
Current research focuses heavily on incremental improvements:
- **Attention Variants**: Sparse attention, local attention, linear attention
- **Training Optimizations**: Better optimizers, curriculum learning, mixture of experts
- **Efficiency Improvements**: Model compression, quantization, distillation

This pattern of refinement-focused research historically precedes paradigm shifts.

### Computational Bottlenecks
Transformers have fundamental limitations:
- **Quadratic Scaling**: Attention mechanisms scale poorly with sequence length
- **Memory Requirements**: Storing attention matrices becomes prohibitive for long sequences
- **Sequential Generation**: Despite parallel training, inference remains largely sequential

## Emerging Paradigms That Could Replace Transformers

### State Space Models
Models like Mamba and S4 are showing promise:
- **Linear Scaling**: Computational complexity that grows linearly with sequence length
- **Long Range Dependencies**: Better handling of very long sequences
- **Parallel Training**: Efficient training while maintaining fast inference

### Retrieval-Augmented Architectures
Instead of storing all knowledge in parameters:
- **External Memory**: Accessing vast databases during inference
- **Compositional Reasoning**: Building complex reasoning from simpler components
- **Dynamic Knowledge**: Updating knowledge without retraining

### Neuromorphic Computing
Brain-inspired architectures gaining momentum:
- **Spiking Neural Networks**: Event-driven computation that could be vastly more efficient
- **Memristive Networks**: Hardware that learns and computes simultaneously
- **Biological Plausibility**: Architectures that mirror actual neural computation

### Hybrid Symbolic-Neural Systems
Combining neural networks with symbolic reasoning:
- **Neuro-Symbolic AI**: Explicit reasoning combined with pattern recognition
- **Program Synthesis**: Learning to write code rather than just pattern match
- **Causal Models**: Understanding causation, not just correlation

## The Economics of Paradigm Shifts

### Why Change is Inevitable
The current trajectory of transformer development faces several economic pressures:

#### Unsustainable Scaling Costs
- **Training Expenses**: GPT-4 reportedly cost over $100 million to train
- **Energy Consumption**: AI training consumes increasing percentages of global electricity
- **Hardware Limitations**: Even with custom chips, we're approaching physical limits

#### Market Saturation
- **Diminishing Differentiation**: As transformer-based models become commoditized, competitive advantage will come from architectural innovation
- **Application-Specific Needs**: Different domains require specialized architectures
- **Edge Computing**: Mobile and IoT applications demand efficiency over raw capability

### The Innovation Cycle
Technology follows predictable patterns:
1. **Breakthrough Phase**: Revolutionary new approach emerges
2. **Improvement Phase**: Incremental enhancements and optimizations
3. **Maturation Phase**: Diminishing returns, focus on efficiency
4. **Disruption Phase**: New paradigm emerges to address limitations

We're clearly in phase 3 with transformers, and early signs of phase 4 are appearing.

## What the Next Paradigm Might Look Like

### Key Characteristics
The successor to transformers will likely feature:

#### Efficiency First
- **Computational Efficiency**: Orders of magnitude improvement in compute per unit performance
- **Energy Efficiency**: Sustainable AI that doesn't require massive data centers
- **Sample Efficiency**: Learning from fewer examples, like humans do

#### Modular Architecture
- **Compositionality**: Building complex behaviors from simpler components
- **Specialization**: Different modules for different types of reasoning
- **Interpretability**: Understanding what each component does

#### Dynamic Adaptation
- **Continual Learning**: Updating knowledge without catastrophic forgetting
- **Meta-Learning**: Learning how to learn new tasks quickly
- **Context Sensitivity**: Adapting behavior based on environment and goals

### Potential Breakthrough Areas
Several research directions could catalyze the next paradigm shift:

#### Biological Inspiration
- **Cortical Columns**: Hierarchical processing inspired by brain organization
- **Attention-Free Architectures**: Learning without explicit attention mechanisms
- **Temporal Dynamics**: Processing sequences through temporal evolution rather than position encoding

#### Mathematical Foundations
- **Category Theory**: More principled approaches to compositionality
- **Information Theory**: Optimizing architectures based on information-theoretic principles
- **Differential Geometry**: Understanding neural networks through geometric lens

## Timeline and Implications

### Near Term (2-5 years)
- Continued transformer refinements with diminishing returns
- Emergence of hybrid architectures combining transformers with other approaches
- Increased focus on efficiency and specialized applications

### Medium Term (5-15 years)
- Clear alternative paradigms gaining traction in specific domains
- Economic pressure forcing adoption of more efficient architectures
- Research community beginning to shift focus

### Long Term (15+ years)
- Transformers relegated to specific niches, like CNNs today
- New paradigm(s) dominating AI research and applications
- Historical perspective recognizing current limitations

## Preparing for the Transition

### For Researchers
- **Diversify Approaches**: Don't put all research eggs in the transformer basket
- **Study Fundamentals**: Deep understanding of principles that transcend specific architectures
- **Cross-Disciplinary Learning**: Insights often come from other fields

### For Practitioners
- **Architecture Agnostic Skills**: Focus on problem-solving rather than tool-specific expertise
- **Efficiency Mindset**: Start optimizing for efficiency now, before it becomes critical
- **Continuous Learning**: Be prepared to adapt to new paradigms

### For Organizations
- **Technology Hedge**: Don't become overly dependent on current transformer architectures
- **Research Investment**: Support fundamental research, not just applied improvements
- **Long-term Thinking**: Plan for technology transitions

## Conclusion

The transformer revolution has been remarkable, but like all technological paradigms, it will eventually be superseded. The signs are already there: diminishing returns on scaling, computational bottlenecks, and increasing focus on incremental improvements over fundamental innovations.

The next paradigm shift won't happen overnight—transformers may dominate for another decade or more. But the seeds of change are already being planted in research labs around the world. State space models, neuromorphic computing, and hybrid architectures are all potential candidates for the next big breakthrough.

The question isn't whether transformers will be replaced, but when and by what. History suggests that the transition, when it comes, will be faster than most expect. Those who recognize the signs early and prepare for the shift will be best positioned to lead the next era of artificial intelligence.

The future of AI agents won't just be bigger transformers—it will be something fundamentally different, more efficient, and more capable than anything we can currently imagine. And that possibility is what makes this field so exciting.
  `
      }
    ],
    previewImage: "/resources/yatsen.jpg"
  },
  quotes: {
    title: "Quotes",
    content: [
      {
        quote: "Capital has the ability to subsume all critiques into itself. Even those who would *critique* capital end up *reinforcing* it instead...", 
        author: "Joyce Messier, from the video game Disco Elysium (Inspired by Herbert Marcuse)" 
      },
      {
        quote: "I don't see where the realism is..", 
        author: "sutha" 
      }
    ],
    previewImage: "/resources/zaporo_cossak.jpg"
  }
};